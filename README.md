## Abstract

```
Manual relevance judgements in Information Retrieval are costly
and require expertise, driving interest in using Large Language
Models (LLMs) for automatic assessment. While LLMs have shown
promise in general web search scenarios, their effectiveness for
evaluating domain-specific search results, such as educational re-
sources, remains unexplored. To explore different ways of including
domain-specific criteria in LLM prompts for relevance judgement,
we collected and release a dataset of 401 human relevance judge-
ments from a user study involving teachers, teacher students and
didactics researchers performing search tasks related to lesson plan-
ning. We compared three approaches to structuring these prompts:
a simple two-aspect evaluation baseline from prior work on using
LLMs as relevance judges, a comprehensive 12-dimensional rubric
derived from educational literature, and criteria directly informed
by the study participants. Using domain-specific frameworks, LLMs
achieved strong agreement with human judgements (Cohen‚Äôs ùúÖ up
to 0.650), significantly outperforming the baseline approach. The
participant-derived framework proved particularly robust, with
GPT-3.5 achieving ùúÖ scores of 0.639 and 0.613 for 10-dimension and
5-dimension versions respectively. System-level evaluation showed
that LLM judgements reliably identified top-performing retrieval
approaches (RBO scores 0.71-0.76) while maintaining reasonable
discrimination between systems (RBO 0.52-0.56). These findings
suggest that LLMs can effectively evaluate educational resources
when prompted with domain-specific criteria, though performance
varies with framework complexity and input structure.
```

## Data

`educational_resource_qrels.csv` contains the QRels for the dataset as a CSV with the columns: `topic_id`, `topic_description`, `doc_url` and `human_judgement`.

The document contents can not be published since they are copyrighted content but local copies can be made by running the playwright script in the crawler directory.

The urls.json file in that directory must first be populated with the `doc_url`s from the CSV. Both screenshots and text should be generated by the script and can be used directly in the analysis notebook.
